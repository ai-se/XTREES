\documentclass{sig-alternate}
\usepackage{tikz}
  \def\firstcircle{(90:1.75cm) circle (2.5cm)}
  \def\secondcircle{(210:1.75cm) circle (2.5cm)}
  \def\thirdcircle{(330:1.75cm) circle (2.5cm)} 
\usepackage{comment}
\usepackage{cite}
\usepackage[shortlabels]{enumitem} 
\usepackage{amsmath}
\usepackage{url}
\usepackage{balance}
\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\setlist{nolistsep,leftmargin=5mm}
%\usepackage[pdftex]{graphicx}
\newcommand{\Sample}{{\bf SAMPLE}}
\newcommand{\PEEKING}{{\bf PEEKING2}}
%\usepackage[table]{xcolor}
\definecolor{darkgreen}{rgb}{0,0.3,0}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\usepackage{colortbl}
\usepackage{picture}

\usepackage{listings}
\usepackage{tabu}
\DeclareMathOperator*{\argmin}{arg\,min} 
\DeclareMathOperator*{\argmax}{arg\,max}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}

\lstset{
    language=Python,
    basicstyle=\ttfamily\fontsize{2.4mm}{0.8em}\selectfont,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=tlrb,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    %backgroundcolor=\color{Gray},
    keywordstyle=\bfseries,
    emph={COCONUT,GUESSES,ASSESS,COCOMO2,PEEKING2,SAMPLE,WHERE,RIG}, emphstyle=\bfseries\color{Blue},
    stringstyle=\color{green!50!black},
    commentstyle=\color{red}\itshape,
    %numbers=none,
    captionpos=t,
    numberstyle=\bfseries\color{red},
    escapeinside={\%*}{*)}
}

\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}

\newcommand{\quart}[4]{\begin{picture}(100,4)%1
{\color{black}\put(#3,2){\circle*{4}}\put(#1,2){\line(1,0){#2}}}\end{picture}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\newenvironment{changed}{\par\color{MyDarkBlue}}{\par}
%\newenvironment{changed}{\par}{\par}

\newcommand{\ADD}[1]{\textcolor{MyDarkBlue}{{\bf #1}}}
%\newcommand{\ADD}[1]{#1}
\usepackage{times}
\def\baselinestretch{1}
\setlist{nosep}
\usepackage[font={small}]{caption, subfig}

\setlength{\abovecaptionskip}{1ex}
\setlength{\belowcaptionskip}{1ex}
\setlength{\floatsep}{1ex}
\setlength{\textfloatsep}{1ex}
\newcommand{\subparagraph}{} % defined before loading titlesec
\usepackage[compact,small]{titlesec}
\DeclareMathSizes{7}{7}{7}{7} 
\bibliographystyle{unsrt}
\pagenumbering{arabic} 
\begin{document}  
\conferenceinfo{ISCE}{'16 Austin, Texas}

\title{Prediction is Hard (Especially About the Future)}
\numberofauthors{2} 
\author{
\alignauthor 
Rahul Krishna \\
       \affaddr{CS, NcState, USA}\\
       {i.m.ralk@gmail.com}
\alignauthor
Tim Menzies\\
       \affaddr{CS, NcState, USA}\\
       {tim.menzies@gmail.com}}
\setlength{\columnsep}{7mm}

\maketitle
\begin{abstract}
This paper provides ...  
\end{abstract}
\section{Introduction}
\section{Cluster and Contrast}
A recurring data analysis pattern in this paper is $cluster+contrast$. The data is distilled into a few clusters using a clustering scheme. Then lessons are inferred by studying the differences between these clusters. These lessons are used to generate rules that can be applied in any context.
\subsection{Clustering}
There are a wide variety of clustering methods to choose from. A study by Ganesan~\cite{div14} explored different clustering methods for software engineering data using the effort and defect data from the PROMISE repository~\cite{promise}. In that study methods such as WHERE, K-Means, mini-batch-K-Means, DBScan, EM, and Ward were investigated. The results of the study showed that the size and number of clusters is more important that the specifics of the techniques used. 

For this purposes of this work, we have chosen WHERE, a clustering scheme which is capable of generating at least $\sqrt{N}$ clusters given $N$ instances. In addition to this, WHERE has been shown to run fast while ignoring spurious dimensions~\cite{menzies2013}. This is particularly useful, for much of the SE data is noisy, they contain a information not associated with the target variable. 

\subsection{Finding Contrasts}
All the following methods use clustering in the form of WHERE, a top-down clustering method which recursively splits the data in two along a dimension that represents the highest variability, shown as Step-1 in figure \ref{fig:xtrees_bare}. 

Clustering is followed generating \textit{contrast sets}. These contrast sets represent recommendations on what could be altered to better improve an outcome. In this work we have explored several algorithms as possible tools to identify contrast between clusters. These fall into three broad categories: a) Case Based Reasoning techniques (Nearest Neighbors), b) A Gradient base planner (called HOW), and c) Decision Trees. These techniques are discussed below. It is worth noting that the following techniques are organized in such a way that each one seeks to address certain fallacies of the ones that precede it. 

\subsubsection{Case Based Reasoning}

Case-based reasoning seeks to find solutions to problems by emulating human recollection and adaptation from past experiences. It has found extensive usage in Artificial Intelligence because it offers several advantages. However, one of the most important benefits that CBR has to offer is that it works on a ``case-by-case'' basis. Therefore it's advise is tailored to be specific to the particular case being considered. Several paper in SE have applied this technique, most of all for effort estimation ~\cite{keung2008analogy, 6600685, walkerden1999empirical, shepperd1997estimating, kocaguneli2010use}. 

A classic example of a CBR is K Nearest Neighbor. Our version of nearest neighbor for planning is rather straight forward. It has been developed as a "straw-man"; i.e., a simple baseline tool to act as a benchmark used to evaluate other methods. 

We divide the $N$ training samples into $\sqrt{N}$ clusters and compute the centroid of these clusters. For every test case, we identify a cluster from the training set that most closely resembles it. Following this, we find the nearest cluster with a better performance score. The differences in the attributes between these two clusters constitute the \textit{"contrast set"}. These contrast sets acts as plans that can be used to reflect over the test cases to improve them.

In most SE applications, not all features contribute equally to a problem. With this in mind, we opine that it would be beneficial if the above method is extended to include some form of feature weighting, thus enabling the tools to recommend changes to only the most informative features. This paper uses a feature weighting scheme similar to that used in CART, see \fig{where}d.

Simple though this method may seem, using nearest neighbors presents the following demerits: a) Displacements are limited to small regions, this implies that large changes may be recommended in order to 
\texttt{<Demrits of knn here and lead to the next\\ section>}

\subsubsection{HOW}

HOW is very different compared to other CBR planners in that it explores the gradient between pairs of nearby clusters instead of studying the clusters themselves. HOW works by clustering the data during training using WHERE and then drawing \texttt{slopes} between the centroids of pairs of nearby clusters. Assuming the cluster pairs are labeled X and Y, with X having slightly better performance score than Y, the \texttt{slope} between X and Y acts as an indicator pointing to a direction to displace the data; i.e. away from Y and towards X. 

While testing, HOW finds the nearest slope to every test case. The slope provides the exact magnitude and direction of displacements. Contrast sets are derived from these displacements. HOW offers a distinct advantage over CBR planners by limiting the displacements to very small regions (the displacements are never more than the separation between two clusters). 

\begin{figure}[t!]
	\small
	~\hrule~
	
	{\bf \fig{where}a: top down clustering:}
	
	Data can be rapidly and recursively divided   as follows.
	Find   two   distance cases,  $X,Y$
	by picking any case $W$ at random, then setting $X$ to its most
	distant case, then setting $Y$ to the case most distant from
	$X$~\cite{fastmap}
	(this requires only $O(2N)$ comparisons
	of $N$ cases).
	Next, HOW projects each case $Z$
	onto a {\tt Slope} that  runs between $X,Y$ using the cosine
	rule of \fig{where}b. After that,  split the data at the median $x$ value of all cases and
	recurses on each half  (stopping when
	one half has less  than $\sqrt{N}$ of the original population).
	
	~\hrule~
	
	{\bf \fig{where}b: finding distances:}
	
	In the \fig{where}a, to compute distances, we use
	the Euclidean measure recommended for
	case-based reasoning by Aha et al.~\cite{aha91};
	i.e. $\sqrt{\sum_iw_i(X_i-Y_i)^2}$ where $X_i,Y_i$
	where values are  normalized 0..1 for the range min..max and 
	$w_i$ is some importance weight given to attribute $i$.
	Usually, $w_i=1$ but it can be adjusted using, say,
	the feature weighting methods of \fig{where}d. 
	
	~\hrule~
	
	{\bf \fig{where}c: 3 point geometry:}
	
	Let   $X,Y$ be two points joined by  a {\em Slope} of  length $c$.
	A third point, $Z$, has distances  $a=dist(Z,X)$ and
	$b=dist(Z,Y)$ to $X,Y$, respectively.
	According to the cosine rule,   $Z$ projects onto  $\overline{XY}$
	at distance $x=(a^2 + c^2 - b^2)/(2c)$ from $X$.
	Further, according to Pythagoras' theorem, $Z$ stands at a distance
	$y = \sqrt{a^2 - x^2}$ from the line $\overline{XY}$. 
	
	~\hrule~
	
	{\bf \fig{where}d: feature weighting:}
	
	HERE's feature weighting algorithm
	comes from the CART regression tree learner~\cite{Breiman1984}.
	It sorts independent variables
	according to how well they can reduce the variability
	of a  numeric objective.
	If we split some column $i$ of independent numeric data  into $N=n_1 + n_2 + ..$ splits,
	then the expected
	value of the objective value of the split  is $w_i = \sum_i v_in_i/N$
	where $v$ is the standard deviation of the objective.
	A recursive procedure can  find those divisions $n_1,n_2,...$ by (a)~sorting a numeric column,
	then split at $\argmin_i w_i$, then recursing into each half.
	
	~\hrule~
	
	\caption{Sub-routines within HOW.}\label{fig:where}
\end{figure}

\subsubsection{Decision Trees}

\section{CROSSTREES}
\subsection{Design}
	\begin{figure}[t]
		\small
		~\hrule~
		
		{\bf Step1: Top down clustering using WHERE}
		
		The data is recursively divided in clusters using WHERE as follows:
		\begin{itemize}
			
			\item Find   two   distance cases,  $X,Y$
			by picking any case $W$ at random, then setting $X$ to its most
			distant case, then setting $Y$ to the case most distant from
			$X$
			(this requires only $O(2N)$ comparisons
			of $N$ cases).
			\item Project each case $Z$
			onto a {\tt Slope} that  runs between $X,Y$ using the cosine
			rule. 
			\item Split the data at the median $X$ value of all cases and
			recurses on each half  (stopping when
			one half has less  than $\sqrt{N}$ of the original population).
		\end{itemize}
		~\hrule~
		
		{\bf Step2: Distinguish between clusters using  decision trees}
		
		Call each leaf from WHERE a  ``class''. Use an entropy-based
		 decision tree (DT) learner to learn what attributes select for each ``class''. To limit tree size:
		 \bi
		 \item Only use the top $\alpha=33$\%  of the features, as determined by their information gain~\cite{Irani1993}. 
		 \item Only build the trees down to  max depth of $\beta=10$.
		 \item Only build subtrees if it contains at least $N^{\gamma=0.5}$ examples (where $N$ is the size of the training set).
\ei
{\em Score}  DT  leaf nodes  via the mean score of its majority cluster. 
		
		~\hrule~
		
		{\bf Step3: Generating contrast sets from DT branches}
		\begin{itemize}
		\item Find the {\em current } cluster: take each test instance, run it down to a leaf in the DT tree.  
		\item Find the {\em desired} cluster: 
		\bi
		\item Starting at {\em current}, ascend the tree $lvl\in \{0,1,2...\}$ levels;
		\item Identify {\em sibling} clusters; i.e. leaf clusters that can be reached from level $lvl$ that are not {\em current }
		\item Using the {\em score} defined above, find the {\em better} siblings; i.e. those with a {\em score} less than $\epsilon=0.5$ times the mean score of {\em current}. If none found, then repeat for $lvl += 1$
		\item  Return the {\em closest} better sibling where distance is measured between the mean centroids of that sibling and {\em current}
		\ei
		\item Find the {\em delta}; i.e. the set difference between  conditions in the DT branch to {\em desired} and {\em current}. To find that delta:
		\bi
		\item
		For discrete attributes,  return the value from {\em desired}. 
		\item
		For  numerics, return the numeric difference. 
		\item
		For numerics  into ranges, return a random number selected from the low and high boundaries of the that range.
	\ei
	\ei
		~\hrule~
		\caption{CROSSTREES. Controlled by the parameters
		$\{\alpha, \beta, \gamma, \delta, \epsilon\}$ (set via engineering judgement).}
		\label{fig:xtrees_bare}
	\end{figure}

\subsection{Assessment}
\section{Experiments}
\subsection{Data}

\begin{figure*}[!t]
\scriptsize
   \begin{center}
   \begin{minipage}{.46\linewidth}
    \begin{tabular}{r@{~}|l@{~}|r@{~}|l@{~}|r@{~}|r@{~}|} \cline{2-6}
   & \multicolumn{5}{c|}{ }\\ 
   
   & \multicolumn{5}{c|}{ Data set  properties}\\ 
   & \multicolumn{5}{c|}{  }\\ 
           & \multicolumn{2}{c|}{training}   & \multicolumn{3}{c|}{testing}      \\ \cline{2-6}
   data set      & versions           & cases & versions     & cases    & \% defective             \\ \hline
        jedit    & 3.2, 4.0, 4.1, 4.2 & 1257      & 4.3          & 492          & 2 \\
        ivy      & 1.1, 1.4           & 352       & 2.0          & 352          & 11 \\
        camel    & 1.0, 1.2, 1.4      & 1819      & 1.6          & 965          & 19 \\
        ant      & 1.3, 1.4, 1.5, 1.6 & 947       & 1.7          & 745          & 22 \\
        synapse  & 1.0, 1.1           & 379       & 1.2          & 256          & 34 \\
        velocity & 1.4, 1.5           & 410       & 1.6          & 229          & 34 \\
        lucene   & 2.0, 2.2           & 442       & 2.4          & 340          & 59 \\
        poi      & 1.5, 2, 2.5        & 936       & 3.0          & 442          & 64 \\
        xerces   & 1.0, 1.2, 1.3      & 1055      & 1.4          & 588          & 74  \\ 
        log4j    & 1.0, 1.1           & 244       & 1.2          & 205          & 92   \\
        xalan    & 2.4, 2.5, 2.6      & 2411      & 2.7          & 909          & 99  \\\hline 
        
        
    \end{tabular}\end{minipage}\begin{minipage}{.4\linewidth}
    \begin{tabular}{|rrr|rrr|rr|l} \cline{1-8}
      \multicolumn{8}{|c|}{  }\\
      \multicolumn{8}{|c|}{  Results from learning}\\
       \multicolumn{8}{|c|}{   }\\
   \multicolumn{3}{|c|}{untuned} & \multicolumn{3}{c|}{tuned} & \multicolumn{2}{c|}{change}\\
  \cline{1-8}
  
  pd & pf & good? & pd & pf & good? & pd & pf\\\cline{1-8}
  55 & 29 &   & 64 & 29 & y & 9 & 0&$\star$\\
  65 & 35 & y & 65 & 28 & y & 0 & -7&$\star$\\
  49 & 31 &   & 56 & 37 &   & 5 & 6\\
  49 & 13 & y & 63 & 16 & y & 14 & 3&$\star$\\
  45 & 19 &   & 47 & 15 &   & 2 & -4\\
  78 & 60 &   & 76 & 60 &   & -2 & 0\\
  56 & 25 &   & 60 & 25 & y & 4 & 0\\
  56 & 31 &   & 60 & 10 & y & 4 & -21&$\star$\\
 30 & 31 &   & 40 & 29 &   & 10 & -2&$\times$\\
  32 & 6 &   & 30 & 6 &   & -2 & 0&$\times$\\
  38 & 9 &   & 47 & 9 &   & 9 & 0&$\times$\\
\hline 
\end{tabular}

\end{minipage}
\end{center}    
  
    \caption{Training and test {\em data set properties} for the Jureczko data sets,
    sorted in ascending order of \% defective examples.
    On the right-hand-side, we show the {\em results from learning}.
    Data is ``good'' if it has recall over 60\% and false alarm under 40\%
(and note that, after tuning, there are more ``good'' than before).
Data   marked with ``$\star$'' show large improvements in performance, after tuning.
Data   marked with ``$\times$'' are ``not good'' since their test suites  have so few non-defective examples (less than 5\% of the total sample) that it becomes harder to find better data towards which we can displace test data.
}\label{fig:j}
\end{figure*}
\subsection{When not to Plan?}
\subsection{Evaluation}
\subsection{Results}
\section{Threats to Validity}
\section{Conclusion}
\section*{Acknowledgements}
\bibliographystyle{plain}

\bibliography{References}
\end{document}

\end{document}
