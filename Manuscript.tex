\documentclass{sig-alternate}
\usepackage{tikz}
  \def\firstcircle{(90:1.75cm) circle (2.5cm)}
  \def\secondcircle{(210:1.75cm) circle (2.5cm)}
  \def\thirdcircle{(330:1.75cm) circle (2.5cm)} 
\usepackage{comment}
\usepackage{cite}
\usepackage{framed,graphicx,xcolor}

 
 
\usepackage{stfloats}

\usepackage[shortlabels]{enumitem} 
\usepackage{amsmath}
\usepackage{url}
\usepackage{balance}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\textsection\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
%\setlist{nolistsep,leftmargin=5mm}
%\usepackage[pdftex]{graphicx}
\usepackage{program}
\newcommand{\Sample}{{\bf SAMPLE}}
\newcommand{\PEEKING}{{\bf PEEKING2}}
%\usepackage[table]{xcolor}
\definecolor{darkgreen}{rgb}{0,0.3,0}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\usepackage{colortbl}
\usepackage{picture}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{listings}
\DeclareMathOperator*{\argmin}{arg\,min} 
\DeclareMathOperator*{\argmax}{arg\,max}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}

\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Comments}{rgb}{0.5,0.5,0.5}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{1,1,1}

 \definecolor{lavenderpink}{rgb}{0.98, 0.68, 0.82}
 \definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}

\newcommand{\quart}[4]{\begin{picture}(100,4)%1
{\color{black}\put(#3,2){\circle*{4}}\put(#1,2){\line(1,0){#2}}}\end{picture}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\newenvironment{changed}{\par\color{MyDarkBlue}}{\par}
%\newenvironment{changed}{\par}{\par}

\newcommand{\ADD}[1]{\textcolor{MyDarkBlue}{{\bf #1}}}
\usepackage{times}
\pagenumbering{arabic} 
\begin{document}  

\definecolor{shadecolor}{gray}{0.9}
\conferenceinfo{ISCE}{'16 Austin, Texas}

\title{How to Learn Useful Changes to Software Projects\\(to Reduce Runtimes and Software Defects)}
\numberofauthors{4} 
\author{  
\alignauthor
Rahul~Krishna\\Tim Menzies, Xipeng~Shen \\
       \affaddr{CS, NC State, USA}\\
       {\{i.m.ralk,~tim.menzies,\\xipengshen\}@gmail.com}
\alignauthor
Andrian Marcus \\
       \affaddr{CS, UT Dallas  } \\ 
       \affaddr{Texas, USA}\\ 
       {amarcus@utdallas.edu}
       \alignauthor
Naveen  \\ Lekkalapudi\\
 \affaddr{Bellhops } \\ 
       \affaddr{Washington D.C.}\\ 
       {navek91@gmail.com}
\alignauthor
Lucas Layman \\
       \affaddr{Fraunhofer CESE  } \\ 
       \affaddr{College Park, USA}\\ 
       {llayman@fc-md.umd.edu}
\setlength{\columnsep}{7mm}
}
\maketitle
\begin{abstract}
 Business users now demand more insightful
 analytics. Instead of just predicting 
 some result, they also need tools that generate ``plans'';
 i.e. specific suggestions on  what to change  in order to
 improve on the predicted values. Hence, we propose the  XTREE planner and  show that
 it runs better than   three alternative methods. For this paper's    case studies,
 XTREE reduced
 the expected values of defects \& runtimes of some software projects to    
 45\% \& 61.5\%  (median) and  22\% \& 6\% (best case) of the initial values, respectively.
\end{abstract}
\section{Introduction}
Business users are demanding  tools that support    business-level interpretations of their data. At a panel on software analytics at ICSE`12, industrial practitioners lamented the state of the art in software analytics~\cite{menzies12a}. Panelists commented  ``prediction is all well and good, but what about decision making?''. Note that these panelists were more interested in the interpretations and follow-up that occurs after the mining, rather than just the mining itself. So:
\bi
\item
Instead of just accepting {\em predictions} on how many software defects to expect, business users might now demand a {\em plan} to reduce the likelihood of those defects.
\item
Instead of just accepting {\em predictions} on the runtime time of their software, business users might now demand a {\em plan} to reduce that runtime. 
\ei
In response to this business-level demands for planners,
we propose a novel {\em planning} approach called XTREE for learning changes to a software system
such that its performance ``improves'', according to some measure. This paper uses XTREE
to reduce  the expected value of the
defects     in Jureczko et al.'s    JAVA systems~\cite{jureczko10};
and the runtimes   in   software    configured by  Siegmund et al.~\cite{sven12}.

The approach discussed here is novel since most data miners used in software analytics
comment on ``what is'' and not ``what to do''. For example, a clustering
algorithm might find clumps of similar examples, but does not comment on the minimal
change(s) required to move from clump to clump.

The contributions of this paper are (1)  
   XTREE   (which is a new algorithm) and (2) an evaluation strategy for   planners.
XTREE     works  on tables of data with weighted class values indicating what rows are ``good'' and what rows are ``bad''.
In the case studies of this paper (reducing    software runtimes and software defects), our evalautions shows
 XTREE performing significantly better than   the planners we proposed in  prior   papers~\cite{me12c,krishna15}.

The rest of this paper is structured as follows.
After describing our test data,
this paper  walks the reader through four methods that start
with a standard clustering algorithm, the  progresses on to XTREE. The second
half of the paper then presents an extensive evaluation of these methods using 
the Jureczko  and Siegmund data sets. This is followed by notes on related work
and validity.

To allow for reproducibility, all scripts and data used in this 
study are available on-line at github.com/ai-se/XTREE.

\section{Preliminaries}\label{sect:prelim}

\subsection{From Prediction to Planning}
 
 
This paper is about the next step {\em after} prediction. Suppose
a business user is presented a prediction and they do not like what they see; e.g. the runtimes are too long of the number of defects is too high. This user may
then ask a {\em planning}  question; i.e. ``what can we change to do better than that?''.

 

Before exploring automatic methods to answer the planning question, we first comment
on two manual methods.

One way to propose changes to a project would be to   ask some smart experienced
person for their opinion on how to (e.g.) reduce defects and/or decrease runtimes. Sometimes
such advice 
is an effective strategy and sometimes it is not.
According to Passos et al.~\cite{passos11},  developers
may  assume that the lessons they learn from a few past
projects are general to 
all their future projects. They comment ``past experiences were taken into account without 
much consideration for their context~\cite{passos11}.  
 J{\o}rgensen \& Gruschke~\cite{jorgensen09} offer  a similar warning. They report that 
  supposed software engineering    ``gurus'' rarely use lessons
  from past projects to improve their future reasoning and that such poor
  past advice can be detrimental to new projects.~\cite{jorgensen09}.
  Accordingly, we   propose a ``trust, but verify'' approach.
  After a software guru offers some sage wisdom,  it is wise to ask some other oracle 
  if there are any better options
  (just as a sanity check).
  The rest of this paper discusses some methods to build automatic oracles 
  to implement that   sanity check.
  
 
Another way  to find   changes to a project
might be to rely
on the peer review processes used by the 
SE research community. This approach would propose changes to software
projects that concur with internationally accept best practices. 
There are two
problems with that approach. Firstly, given the rapid pace of change in software
engineering, we may be asking questions for which there is no currently accept
``best practice''. For example, only very recently has there been any work
of predicting software runtimes based on choices within Makefiles. While the work
of Siegmund et al.~\cite{sven12} is certainly state of the art, it does
cannot yet represent ``internationally accept best practices'' since that work is so
recent. 

Secondly, given the diversity of SE products and practices
and personnel, it may well be that the current project being discussed is 
substantively different to prior work. 
Numerous recent {\em local learning} results compare (1) single models
learned from all available data to (2) multiple models learned from clusters within the data~\cite{betten14,yang11,yang13,minku13,me12d,me11m,posnett11}.
A repeated result in those studies in that the local models generated the better effort
and defect predictions (better median results,
lower variance in the predictions). This paper offers yet another locality result:
\bi
\item
One standard rule in the literature
is that it is useful to implement modules such that they are internally cohesive (use
much of their own local methods) while being loosely coupled with other classes~\cite{Dhama199565}.
\item
While that may be true in general, for particular classes other changes may be more important
(later in this paper, we show one set of results were that is indeed the case).
\ei
In summary, 
it is useful to have automatic methods to recommend changes. Such
methods can fill in for human gurus (if such gurus are absent) or 
to offer a second opinion.
Also, prior to making automatic recommendations, it is wise to first stratify the data
(clump it into related examples) then generate advice specific to each clump.
Accordingly, the rest of this paper defines and evaluates
automatic methods to find plans from
  $N$ examples divided  into  many clumps.


\input{tex/jur_char.tex}


\subsection{Trusting the Changes}\label{sect:trust}
   XTREE is evaluated by  comparing
predicted performance scores before and after a planner makes changes to the feature values of an example:
After making those
changes, we may have a new example that has never seen before. Therefore, it must be asked:
\bi
\item
{\em Can we trust the predictions made on such new examples?}
\ei
To answer this question, we note
that data miners explore two  ``clouds'' of data: (1) the cloud of training examples and (2) the  cloud   of test examples.
For a visualization of these clouds, see \fig{howxy}.


\begin{figure}[!t]
  \includegraphics[width=\linewidth]{figs/twodee.eps} 
 % \includegraphics[width=1\linewidth]{figs/BDBC.eps}
\caption{Training examples (in gray), test examples (in red) and 
changed examples generated by a planner (in green).
Displayed on the first two components of a PCA analysis of all points. 
The green (changed) examples are closer to the  training
date than the original test data
i.e. predictors trained on the training examples and
applied to the test examples should
work just as well, or better, on the changed test examples.}\label{fig:howxy}
\end{figure}


We should mistrust the predictions made by a  model   if it is being applied to examples  that are
too far away from the
training cloud.
To test for ``too far'', we can run a data mining experiment that tests how well
a model learned from the training data applies to the test data. Such experiments return some performance value.

Note that predictions  about changes that  fall within the space of the training+test data, will be at least
as reliable as the performance value found in the above data mining experiment.
Given this, one thing  can be asserted about predictions on changed examples:
\bi
\item Predictions for changes that move examples towards/away from the training data can be trusted more/less (respectively).
\ei  
Accordingly, we should use  {\em trust-increasing} planners that generate changed examples {\em closer} to the
training examples.  To see how this works, 
 \fig{howxy} is from the {\em ivy} data
set, which is one of the Jureczko data sets explored in this paper. It shows: (1)~the training examples in gray, (2)~the test examples in red, and (3)~the
changed  examples displaced after applying a plan (in green).
 Note that the  the   changed examples
cases  (shown in green)  fall closer to the training cases (shown in gray) than
the test cases (shown in red). 

In that green region of changed examples, our belief in the value of predictions
will be as much (or more) as our belief in the value of the predictions in the red region (that
contains the original test data).
This pattern of \fig{howxy} (where the changes examples are found closer to  the training cases than the test cases) has been observed in all the other data sets studied in this
paper. Hence,  we can assert that
predictors learned from these training examples have some authority in the regions
contain the changes examples.


That said, the above comes with some important caveats:
\bi
\item 
We   strongly recommend that predictors are assessed prior to planning. That
issue is explored further in the \tion{tesd}.
\item
Planners should be designed to be {\em trust increasing}. We list four such planning methods in \tion{planners}.
\item
Where possible, planners should be assessed via some external
oracle that can accurately assess new examples. For an example of that kind of analysis,
see  \tion{valid}.
\ei

%XXX end


  
 \begin{figure}[!t]
 \small
 \begin{center}
 \begin{tabular}{r|rr}
 data set & cases & \% defective\\\hline
  ant &947& 22\\
  camel& 1819& 19\\
 jedit& 1257& 2\\
 ivy &352& 11\\
 log4j& 244 &92\\
 lucebe &442 &59\\
 poi& 936 &64\\
 synapse &379 &34\\
 velocity& 410& 34\\
 xalan& 2411& 99\\
 xerces &1055& 74
 \end{tabular}
 \end{center}
 \caption{ Jureczko data: columns in the format of \fig{ck}.}\label{fig:jd}
 \end{figure}
 
 \begin{figure}[!t]
\scriptsize
\begin{tabular}{llllll}
  \hline
  \rowcolor{lightgray}
Project & Domain & Lang. & LOC & Features & Config\\\hline
BDBC: Berkeley DB   & Database & C & 219,811 & 18 & 2560\\
BDBJ: Berkeley DB   & Database & Java & 42,596 & 32  & 400\\
Apache & Web Server & C & 230,277 & 9 & 192\\
SQLite & Database & C & 312,625 & 39 & 3,932,160\\
LLVM & Compiler & C++ & 47,549 & 11 & 1024\\
x264 & Video Enc. & C& 45,743 & 16 & 1152\\\hline
\end{tabular}
 
\caption{Siegmund data.
For SQLite, the data  contains 4,553 configurations for prediction modeling and 100 additional random configurations for prediction evaluation, see \cite{vapp}.}\label{fig:cpm}
\end{figure}

\begin{figure}[!t]
\includegraphics[width=1\linewidth]{figs/BDBC.eps}
\caption{ Berkeley database feature model   (``C'' version). }\label{fig:bdbc}
\end{figure}


  \section{Test Data}\label{sect:tesd}
To assess our planning methods, we use data from Jureczko et al.'s object-oriented JAVA systems~\cite{jureczko10}  and
  software system   configuration data from by  Siegmund et al.~\cite{sven12}.
  See github.com/ai-se/XTREE\#data for full access to this data.
  
  
   The Jurecko data records number of known defects for each class, where the classes are described in terms of
  nearly two dozen metrics such as number of children, lines of code, etc. For details on the Jurecko data, see  \fig{ck} and \fig{jd}. 
 For the most part, the methods of this paper treat
  the Jurecko as a discrete class data set, where {\em defects} are true if the raw defect count is greater than zero.
  The one exception will be the `best-in-cluster'' method that reflects on the numeric value of the raw defect count (see \tion{BIC}).

  The Siegmund data, described in \fig{cpm},  records  the runtimes of compiled systems. To make that data, Siegmund et al. perturbed
  the configuration parameters in the Makefiles of six systems: Apache, SQLite, LLVM, x264 and two versions of the
  Berkeley database (one written in ``C'' and one in Java). 
   \fig{bdbc} shows an example of a feature model defining valid combinations of settings to on the
   the Siegmund et al. datasets. These feature models were used by Siegmund et al. to ensure all their perturbations are value
   (we will use the same models to cull invalid plans).
  Given those valid perturbations, the systems were then compiled and 
  Siegmund et al. recorded how long each perturbation took to run a test suite. 
  
  
\input{tex/prediction_stats.tex}

  Our evaluation strategy (discussed below) divides this data into a training a test set.
  From the train set we apply a data miner (to learn a quality predictor) and
  various planning methods (to learn different plans). Next, we try applying
  each of those plans to the  test set and ask the quality predictor to assess the changed examples.
  Finally, we say that the  ``best'' planner is the one that most reduces the predicted values
  in the changed examples.
  
  As mentioned in the last section,  this approach depends on having effective predictors for assessing the results.
  For the Siegmind data, this criteria was   relatively easy to achieve.
  The data in those data sets have a continuous class (runtime of the compiled system)
  so the performance of a quality predictor can  be measured in terms of  difference between the predicted runtime $p$ of test case items and their actual runtimes $a$ using  $s= 1 - \frac{abs(a - p)}{a}$ (and {\em higher} values are {\em better}).
This paper  explores six Siegmund configuration data sets:  Berkeley DB (Java and C versions), Apache, SQLite, LLVM, and
  x264. 
  As a preliminary study, we split that data   into equal sized train:test groups
  and trained a Random Forest
  Regressor (from the SciKit learn kit~\cite{Pedregosa2012})   on one half, then applied to the other. This  achieved nearly perfect scores of $s=\{99.9, 99.8, 99.4, 99.1, 96.1\}\%$.
That is, we can be very confident that the predictors from the Siegmund data can assess
our plans. (Aside: if the reader doubts that such high scores are achievable, we note that these scores are consistent with those achieved by predictors built by Siegmund et al.~\cite{sven12}.)



 It proved to be  more complicated to commission the Jureczko data sets for this study.
 For that data, we found that the
 quality predictors built from this data are far from perfect;
However, for some data sets, the  predictors could
be salvaged using the techniques discussed in this section.

 \fig{j} shows our preliminary studies with the Jureczko   data.
Given access to $V$ released
versions, we test on version $V$ and train on the available data from $V-1$ earlier releases (as
shown in \fig{j}, this means that we are training on hundreds to thousands
of classes and testing on smaller test suites).
Note the   \colorbox{lavenderpink}{three bottom rows}   marked with $\times$: these contain predominately
defective classes (two-thirds, or more).  In such data sets, it is hard to distinguish
good from bad (since there are so many bad examples). 


The  Jureczko data uses non-numeric discrete classes (``defective'' or ``not'').
For such data, quality predictor   is be measured using
(1) the  probability of detection (a.k.a. ``pd'' or recall):  the percent of faulty classes in
the test data detected
by the {\em predictor}; and (2) the 
probability of false alarm (a.k.a. ``pf''): the percent of non-fault
classes that are {\em predicted} to be defective.

As a preliminary study, we split the Jureczko  data   into equal sized train:test groups.
Random Forests (again, from the SciKit learn kit~\cite{Pedregosa2012}) were
built from the training data, then applied to the test data.
The ``untuned'' columns of \fig{j} shows those results.
If we define ``good'' to mean $\mathit{pd}>60 \wedge \mathit{pf} < 40$\%,
then only two of our data sets ({\em ivy,ant}) are ``good'' enough for this study.
Note that, as might have been expected, none of the \colorbox{lavenderpink}{three bottom rows} of \fig{j} were ``good''.

Fortunately,
the ``tuned'' columns of \fig{j} show that we can salvage some of the data sets. Pelayo and Dick~\cite{pelayo07} report that defect prediction is improved by SMOTE~\cite{Chawla2002}; i.e. an over-sampling of minority-class examples. Also, Fu et al.~\cite{fu:ase15} report that parameter tuning with differential evolution~\cite{storn97} can quickly explore the tuning options of Random Forest to find better settings for the (e.g.) size of the forest, the termination criteria
for tree generation, etc. The rows \colorbox{celadon}{marked with a $\star$} in \fig{j} show data sets whose performance was improved remarkably by these techniques. For example, in {\em poi}, the recall increased by 4\% while the false alarm rate dropped by 21\%. However,  as might have been expected, we could not salvage the data sets in the  three bottom rows.

In summary, while we cannot trust predictors from some of our defect data sets,
we can plan ways to reduce defects in {\em jedit, ivy, ant, lucene} and {\em poi}.
Accordingly, when this study explores the Jureczko data, we will use these five data sets.

(Aside: One important detail to be stressed here is that, when we applied    SMOTE-ing and
parameter tunings, those techniques were applied to the training data and {\em not}
the test data; i.e. we took care that no clues from the test set were ever used in this tuning process.)



 
\section{Four Planning Methods}\label{sect:planners}
 
In his textbook on empirical methods, Cohen~\cite{cohen95} advises that any supposedly better
algorithm should be baselined against simpler alternatives. Accordingly,
this section described XTREE (which we call Method4) against three  
alternatives.
  XTREE  uses the decision tree learner of \fig{where}.D.  It is new to this paper.
  The other methods use the   top-down
	bi-clustering method described in \fig{where}.C  which recursively divides the
	data in two  using a dimension that captures the greatest variability in the data. 
	We proposed Method 1 and 2   in 2012~\cite{me12c} while Methods 3 comes from research conducted earlier this year~\cite{krishna15}.
 
 \subsection{  Method Properties}
  Note that all   methods have the  properties proposed in \tion{prelim}:
  \bi
  \item They are {\em local learners}; i.e. different test examples
  will be given plans that are specialized  to their particulars;
  \item They  are {\em trust-increasing};
  i.e. they change  examples such that they move {\em closer} to the training data.
  \ei

 \subsection{  Methods}

Our  description of the methods adopts the following convention. All variables
  set via  our engineering judgement  with Greek letters; e.g. $\alpha,\beta,\gamma$.
  In this paper, we show our current settings to these variables produces useful
  results. Elsewhere\cite{krall14,fu:ase15}, we are exploring tuning methods to 
  find better settings but  we have nothing definitive yet to report
  on auto-tuning planners.

\subsubsection{Method1= CD=   Centroid Deltas}


 {\em Summary1:} Method1  computes a plan from the difference between where you are  (which we will call $C_i$) and
where you want to be  (which we will call $C_j$).

{\em Assumption1: } Method1 assumes that large data sets can be adequately represented by a few dozen (or so) 
centroids.
 
 {\em Details1:} Method1 clusters project data by reflecting   on the independent variables, then
  reports the delta between the cluster centroids. 
  After clustering   training data using the WHERE algorithm of  \fig{where}.C, Method1
replaces all clusters with a  centroid $C_i$ computed from the mean/mode value of each
continuous/discrete feature. After that, it
finds the closest centroid $C_j$ that has a better
performance score. For defect data, ``better'' means fewer defective examples while for the config data,
``better'' means lower median runtimes for the examples in that cluster.
Method1 then caches the  delta between the independent features between $C_i$ and $C_j$. For continuous
features, this delta is $C_j - C_i$. For discrete values, this delta is the value of that feature
in $C_j$. 
Finally, for every test case, Method1 use the distance measure $d$ shown in \fig{where}.B to find
the nearest centroid $C_i$.  It then proposes a plan for improving that test case
that is the conjunction of all the deltas between $C_i$ and $C_j$.


\subsubsection{Method2=CD+FS=Method1+Feature Selection }
 
{\em Summary2:} Method2 works line Method1 but now the   plans  only
mention the $\beta=33\%$ most informative features. Hence, Method2's plans are simpler.

{\em Assumption2: } Method2 assumes that, when reasoning about centroids, we can
just use   features
that   best distinguish    centroids; i.e. whose values appear in just a few centroids.
 

{\em Details2:} A common result is that the signal in a table of data is mostly contained in a handful of features~\cite{hall03,kohavi97}.
Papakroni~\cite{papa13} has tested for this effect in the Jureczko data sets.
Papakroni found no loss of   efficacy in defect prediction after
sorting all features by their information content,
then making predictions using (a)~all  features or (b)~just using   33\% most informative features.

Based on the above, it might be possible to simplify the plans found by Method1  by pruning back the features in those
 plans. Following on from Papakroni, our Method2 returns plans
containing just the top $\beta=33\%$ most informative features. Here, ``informative'' means
that the values of a feature are good for selecting a small set of clusters (ideally,
just one).
This can be estimated using the Fayyad-Iranni INFOGAIN algorithm~\cite{FayIra93Multi}
of \fig{where}.E.
 
 
 \input{tex/algos}
\subsubsection{Method3= BIC=   Method2 + Best-in-cluster}\label{sect:BIC}

{\em Summary3:} Method3 is like Method2, but it uses more knowledge about the training data.

{\em Assumption3: } Method3 assumes that there exists ``gradients'' between and
within clusters which, if used, will better guide us to finding better plans.

{\em Details3:}
Method3 summarizes clusters into {\em two} examples: (1)~the centroid $C_x$ found in Method1 and
(2)~the best-in-cluster $B_x$  example; i.e. the  example in that cluster
with the best performance score. 
For the Siegmud data, $B_x$ is the cluster member with the fastest runtime;
for the Jureczko data, $B_x$  is the cluster member with lowest raw defect count
(resolving ties at random).

Method3 connects  each centroid to a nearest neighbor
by {\em gradient}.
Each gradient has a (bottom,top) end labelled  ($C_i,C_j$) containing the  (worst,best) centroid performance scores, respectively..  
For each test instance, Method3 
find the nearest gradient, the runs up to the top  best end $C_j$, then extracts $B_j$ (which is the
 best-in-cluster associated with  $C_j$).
The returned plan is then computed  from the delta between the test case
and $B_j$.
 
%\input{tex/pseudo-code-1.tex}


\input{tex/dtree.tex}

\subsubsection{Method4=XTREE=  Deltas in Decision   Branches}

{\em Summary4:} Method4 builds a decision tree,  then generates
plans from the difference between two branches:
(1)~the branch to where you are and (2)~the branch to where you want to be.

{\em Assumption4:} One potential problem with Methods 1,2 and 3 is the {\em unsupervised} nature of
the algorithm used for the initial clustering. Unsupervised methods like WHERE
execute without knowledge of the target class.  {\em Supervised} methods, on the other hand, assume that it is useful to also reflect on the target class.

{\em Details4:} 
XTREE uses a supervised   decision tree algorithm of \fig{where}.D to divide the data.
Next, XTREE builds plans from the branches of the decision trees. It does so by asking three questions:
\be
\item
What {\em current} branch does a test case fall in?
\item What {\em desired} branch would the test case want to move to?
\item What are the {\em deltas} between current and desired? 
\ee
The last question can be answered by finding the deltas in branches of the decision tree that lead to {\em desired} from {\em current}. For full algorithmic details, refer to \fig{xtrees_bare}.

\section{Experiments}

  
 

This section describes an experimental design (and results) for evaluating the above four methods. 
\subsection{Experimental Design}

\subsubsection{A Strategy for Evaluating Planners}
 
 Here is our experimental design:
 
 \begin{figure}[!h]
{\small 
\[
\begin{array}{r} 
\mathrm{project}\\
\mathrm{data}
\end{array} 
\left\{\begin{array}{l}\mathit{train}
        \left\{\begin{array}{l}
                \mathrm{learn\;a\;}\mathrm{predictor\;}\mathrm{(e.g.\;via \;Random\;Forest)}\\
                \mathrm{learn\;a\;}\mathrm{planner\;}\mathrm{(e.g.\;via \; XTREE)}
              \end{array}\right.
       \\
      ~\\
\mathit{test}  
    \left\{\begin{array}{l@{~}l}
           \mathit{before}& =\mathrm{Performance\; scores \hspace{2pt} in \hspace{2pt}}\mathit{test}\\
           \mathrm{\bf if\;}\mathit{before} & >  \mathit{0}\\
           \mathrm{\bf then} &
           \left\{
            \begin{array}{l}
                \mathit{test'} = \mathrm{planner}(\mathit{test})\\
                \mathit{after} =\mathrm{predictor}(\mathit{test'})\\ 
                \mathrm{{\bf return}\;} R=\frac{\mathit{after}}{\mathit{before}}
            \end{array}
          \right.
   \end{array}\right.
\end{array} \right. 
\]}
 \caption{Experimental .}\label{fig:design}
 \end{figure}

As shown in \fig{design}, we divide the
project data  into two disjoint sets {\em train} and {\em test}
(so \mbox{{\em train} $\cap ${\em test} $=\;\emptyset$}).
Next, from the train set, we build both a {\em planner} and
 a {\em  predictor}. 

Our general framework does not   commit to any particular choice of { planner} or { predictor} but, for the purposes of this paper:
\bi
\item Our {\em planner} will be one of Methods 1,2,3,4;
\item Our  {\em predictor} will be the Random Forest Classifier~\cite{Breiman2001} (for discrete classes) and Random Forest Regressor (for continuous classes) taken from  SciKit Learn~\cite{Pedregosa2012}.   We use these
data miners since extensive studies have shown these to be amongst the better alternatives for mining software data~\cite{lessmann}.
\ei
As for the {\em test} data, this is passed to the { predictor}
to measure performance statistics related to effectiveness. 

If our { predictors} fail to perform effectively on the test data,
then we cannot trust them to comment on our plans. Accordingly,
if that performance is unsatisfactory, we abort. Recall from \tion{tesd} that this step indicated
we should not use some of the  Jureczko data.

Else, we (1)~apply the { planner} to alter the {\em test} data;
then (2)~apply the { predictor} to the altered data $test'$;
then (3)~return data on the {\em before, after} predictions expressed as the ratio $R=\frac{\mathit{after}}{\mathit{before}}$.
This is a unit-less ratio with the following properties:
\bi
\item If $R= 1$, this means  ``no change from baseline''; 
\item If $R < 1$, this indicates ``some reduction to the baseline'';
\item If $R > 1$, this indicates ``optimization failure''.
\ei
 \subsubsection{Statistical Methods}
 Our methods use some stochastic algorithms; e.g. WHERE's selection of ``what example to explore first'' (see \fig{where}.C) and
  XTREE' occasional use of a random guess when deciding what part of a discretized range to include in the plan
  (see \fig{xtree_bare}). Hence, we report the $R$ values seen in 40 repeated runs
  (with different random number seeds).
The the value 40 was chosen to be  larger than the 30 samples  required
to satisfy the central limit theorem.

  To rank our methods using the results from these 40
  repeats, we use the Scott-Knott test recommended by Mittas and  Angelis~\cite{mittas13}. 
In that test, using the median values of each method,  
sort a list of  $l=40$ values of $R$ values found in  $ls=4$ different methods. 
Then,
splits $l$ into sub-lists $m,n$ in order to maximize the expected value of
 differences  in the observed performances
before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$:
 \[E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2\]
Scott-Knott  applies an statistical  hypothesis test $H$ to check
if $m,n$ are significantly different  (in our case, the conjunction of A12 and bootstrapping). 
If so, Scott-Knott  recurses on the splits.
In other words, the Scott-Knott procedure being used here divides the data if \textit{both} bootstrap sampling and effect size test agree that a division is statistically significant (with a confidence of 99\%) and not a small effect ($A12 \ge 0.6$).

For a justification of the use of non-parametric bootstrapping, see Efron \& Tibshirani~\cite[p220-223]{efron93}. For a justification of the use of effect size tests see Shepperd\&MacDonell~\cite{shepperd12a}; Kampenes~\cite{kampenes07}; and Kocaguenli et al.~\cite{Kocaguneli2013:ep}. These researchers warn that even if an hypothesis test declares two populations to be ``significantly'' different, then that result is misleading if the ``effect size'' is very small. Hence, to assess the performance differences we first must rule out small effects using A12, a test   recently endorsed by Arcuri and Briand at ICSE'11~\cite{arcuri11}.



\subsubsection{Report Format}

   
Our results are presented by the line diagrams like those shown on the right-hand-side of the following example table.
The black dot shows the median $R$ value and the horizontal likes stretch over the inter-quartile
range (hereafter, IQR) that is the space from the 25th percentile value to
the 75th percentile value.

\begin{center}

{\small  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~}c@{}r}} 
\arrayrulecolor{lightgray}
\textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\\hline
1 &         XTREE &    0.49  &  0.13 & \quart{7}{25}{17}{115} \\\hline  
2 &      CD &    0.59  &  0.18 & \quart{15}{34}{36}{115} \\
2 &          BIC &    0.60  &  0.12 & \quart{24}{24}{38}{115} \\\hline  
3 &      CD+FS &    0.62  &  0.06 & \quart{36}{12}{42}{115}  \\\hline \end{tabular}}
\end{center}

In this example table, the rows are  sorted on the median values of each method. Note that all the methods
have   $R<1$ values; i.e. all these methods reduced the expected value of the performance score in that experiment
while XTREE achieved the greatest reduction (down to 49\% of the original value).


The above eample table has a  left-hand-side  {\bf Rank} column, computed using the
Scott-Knott test described above. This column reports if
the values for each method are statistically different and are more than trivially different. 
In this example table, CD and BIC are ranked together while XTREE and CD+FS are ranked best and worst, respectively.
  
 

 
\subsubsection{Other Details}
 
\fig{jur} and \fig{conf1} show the effectiveness of our methods seen in 40 repeats with each data set.
In these experiments,   the dependent variables of Jureczko and Siegmund data sets are discrete and continuous in nature, respectively. Hence, while choosing the predictor, we used Random Forest (1) as a classifier for Jureczko data and (2) as a regressor for Siegmund data.
\bi
\item For Siegmund data, we randomized the order of the data, training on one half while identifying treatment plans on the remaining test data. 
\item For the Jureczko data, we used the training and testing sets of \fig{j}. For these datasets,
 all the SMOTE-ing and Random Forst tunings (discussed in \tion{tesd})
occurred in the {\em train} phase of \fig{design}.
\ei
\newpage\subsection{Experimental Results}

% \begin{figure}[t]
% \begin{minipage}{0.5\textwidth}
% \includegraphics[width=0.9\linewidth]{figs/Deltas-Lucene.eps}
% \label{fig:delta_ivy}
% \end{minipage}
% \begin{minipage}{0.5\textwidth}
% \includegraphics[width=0.9\linewidth]{figs/Deltas-BDBJ.eps}
% \label{fig:delta_apache}
% \end{minipage}
% \caption{Performance Comparison showing the number of times a certain feature requires to be changed. X-axis lists the various features that make up a data set. The y-axis contains the frequency of change in \%.}\label{fig:changed}
% \end{figure}

 
 
\input{tex/results_jur.tex}
\input{tex/results_conf.tex}




\begin{figure*}[bp]
\centering
\includegraphics[width=\linewidth]{figs/Deltas-both.eps}
\caption{Performance Comparison showing the number of times a certain feature requires to be changed. X-axis lists the various features that make up a data set. The y-axis contains the frequency of change in \%.}\label{fig:changed}
\end{figure*}

\subsubsection{Effectiveness Results}

For a planning system designed for business users, we seek two properties:
\bi
\item {\em Effectiveness:}    i.e.  if  its recommendations  are  applied  then  some  statistically significant  change  should  be  observed. 
\item {\em Succinctness:} given two plans with the same effectiveness, we prefer the shorter one.
Such shorter plans are easier to explain (less to show) and faster to implement (less to change).
\ei 
Measured in terms of effectiveness,
some data sets were harder to optimize that others.  SQL (in \fig{conf1}) defied all 
our methods for reducing runtimes. Also, BDBJ (in \fig{conf1}) was hard to optimize
for all methods except XTREE.  However, in other data sets,
large reductions were observed:
\bi
\item
Down to 22\% of the original baseline in Ant of \fig{jur};
\item
Down to 6\% of the original baseline in BDBD  of \fig{conf1};
\ei
Overall, XTREE was most effective. It was always  the top-ranked method and (with 
the exception of SQL), had significant reductions in the median performance
values: median improvement of least 10\% lower (i.e. better) than the next ranked method



\subsubsection{Succinctness Results}



\fig{changed} reports the percent of times in the 40 repeats that a method proposed changing a feature.
The left-hand-side plot of that figure reports results from one of the  Jureczko data sets ({\em lucene}) and the right-hand-side
  shows a Siegmund data set ({\em BDBJ}). 
  
  In these plots, the {\em more} succinct a planning method, the {\em less} percent
  of the runs
  where 
  it   recommends changing a particular feature (i.e. the vertical bars in that plot
  are {\em lower}). For example,
 Method1 (CD) was the least succinct  since it  wanted to change all features 
 (observe the change frequencies as 
high as 100\% for all features). 
Method1's policy of ``change everything''  might be acceptable
if this approach lead 
to the most effective changes. However, looking at 
\fig{jur} and  \fig{conf1}, there is no evidence for this.  

An  interesting feature of \fig{changed} was that fewer things 
were changed in the config data sets {\em BDBJ} than in the defect data set
{\em lucene}. 
In turns out that this holds true across nearly all our data sets.
\fig{types} summarizes all the change frequencies for all data sets. As with  \fig{changed},
there are fewer changed features in the config data than in the defect prediction data. 
One explanation for that is the nature of the features: the
config data has binary independent features while the defect data sets has continuous
features. When exploring these different data types, it is possible to find more
``gentle slopes'' the lead to small changes
in the continuous features and more ``sharp cliffs'' that lead to major change in the discrete
data. Hence, our planners make fewer larger changes in the config data.
offer 

Looing across the columns of \fig{types}, we see that Method3 (BIC) is the most
succinct for data sets with deiscrete features (


\input{tex/deltas_avg.tex}

Method2 (CD+FSS) was more succinct than Method1 
since the feature selection mechanism, removed most of
the feature (evidence: the blue bars for CD+FSS appear least often in both plots). One
repeated pattern in these results was that 
when CD+FSS decided that a feature needs to be changed, it always changed it in all repoeats (see how the blue bars for CD+FSS are usually at 100\%). We conjecture that this is a side-effect
of feature selection-- if you discard most of the controllables, then you always have to change the
remaining.

In terms of succinctness,  Method3 (BIC)  had different  nearly as unsuccinct as Method1 (see in those results
that most features have black bars that extend up beyonf 50\% and often to near 100\%). 
Method3 was more succinct for {\em BDBJ}




As predicted in the above discussion on Method 1 (CD), this method always proposes changes  to most features. Method3 (BIC) often proposes changes to many features, which 
would make its plan tedious to implement. 
Method 2 (CD+FS)
proposes less, due to its use of feature selection and, in the case of {\em BDBJ}, it fixates on changing just one feature (which
probably explains why CD+FS performs so poorly for {\em BDBJ} in \fig{conf}). Finally, Method4 (XTREE) rarely fixates
on changing one feature (only on {\em LUCENE}'s lines of code feature) and seems to prefer making many small changes,
all over the space of features.

\fig{changed} reports the changes just for   {\em lucene}) and {\em BDBJ}. \fig{types} shows the mean changes for features found by our
four methods (here, mean change is the average height of the histogram bars seen in, say, \fig{changed}). Overall, Method1 (CD) and
Method3 (BIC) make the most changes while XTREE and CD+FS make the fewest.



\subsection{Discussion}


These results let us comment on the assumptions of the various planning methods:
\bi
\item Assumption1: 
\item Assumption2: 
\item Assumption3:
\item Assumption4:
In all these results,  with one exception,  The  exception is SQL which (as shown in \fig{conf}), seems to defy all methods for optimization
so XTREE can only reduce the expected values of the runtimes by 1\%.
\ei
 
\section{Validity}\label{sect:valid}
	
\section{Related work}

mockus

planning

harman and change

cbr

results suggest that, when clustering on independent variables of software project data,
	we are free to select from a wide range of 
	clustering methods.  Ganesan~\cite{div14} explored 
	different clustering methods for SE data using   effort and defect data from
	the PROMISE repository~\cite{promiserepo}.
	That study explored
	K-Means, mini-batch-K-Means, DBscan, EM, Ward, and the WHERE algorithm discussed
	below.
	Clusters were assessed via the performance of prediction 
	learned from the clusters by Random Forests (for defect prediction)
	and M5prime or linear regression (for effect estimation).  Ganesan found
	that the details of the clustering method were less important than ensuring that  a large number of clusters were generated.
	Accordingly, for clustering, we select a clustering method that generates a large
	number of small cluster
	($\sqrt{N}$ clusters
	from $N$ examples) that runs fasts, which has shown promise in prior work~\cite{Menzies2013}, and which ignores spurious dimensions (the last item is important since, in our experience, much SE data is ``noisey''; i.e. contains signals not associated with the target variable.
	ats to Validity


\input{tex/results_models.tex}



\section{Related Work}
dashboards

strucftureal learners (bayes)

 

Many data mining methods offer a succinct summary of project data
e.g. see the Bayes nets
favored by Dejaeger et al.~\cite{Dejaeger13}; or   the regression models
learned by Nagappan~\cite{Nagappan05}.
Another  technique is to offer a dashboard that visualizes project data.
For example, the dashboards
provided by the Hackystat team~\cite{Johnson09} presents extensive and detailed
summaries of programmer behavior.

Some users do not like being told what to do... they prefer to have the current world explained to them 
-- but those dashboards typically do not learn threshold points
(that report where action is needed) nor do they offer specific advice on what actions to take.


\section{Conclusion}

works for more than defects and tunrimes

This main contribution of this paper was to  define an evaluation strategy for this kind of reasoning, and to show that (using that strategy) we
can (a)~select useful planning methods while (b)~culling clearly
inferior planning methods. 
We apply this strategy to XTREE and three alternate planning methods to find:
\bi
\item Two methods that are clearly inferior;
\item One method that proposes complex changes, which work as well as XTREE;
\item And XTREE, that proposes very simple changes.
\ei
Of course, there are many more methods for generating plans and
no   one paper can survey them all.
But the goal of this paper is not to claim that (e.g.) XTREE is some absolute optimal algorithm. Rather, it is
to offer a baseline result (with XTREE) and an  evaluation strategy that  can assess  if alternate methods are better than XTREE.
Our hope is that other   researchers apply this strategy  to repeat and/or improve
and/or refute our results. To that end, all our materials and data sets are available on-line, see
github.com/ai-se/XTREE.


\section*{Acknowledgements}
The work has partially funded by a National Science Foundation CISE CCF award \#1506586.
\bibliographystyle{plain}

\bibliography{References}
\end{document}


  
  Next, build one centroid for each cluster (using the median and mode value for continuous
and discrete values, respectively).
After that, write a spreadsheet with one column per centroid;
\bi
\item
Sort the columns such that any current projects client fall into the left-most
columns. That is, make the left-hand-side of the sheet  ``current usual practice''
and the right-hand side  ``alternatives to current usual practice''.
\item
Sort the rows of that spreadsheet such that the rows with
maximum  variability appear at the top 
\ei
Now show the sheet to the business user, encouraging them to make recommendations by
\bi
\item
Comparing the left and right-sided columns;
\item
Using the features mentioned in the top-most rows (where changes selects for the most different centroids).
\ei

		
	
		

In theory, One  advantage of this method is that it focuses the attention of the business users
away from rows that do not select for different centroids and towards differences between
current usual practice and everything else.  
In the summer of 2011 and 2012, one of us (Menzies) spent two months
	working on-site at Microsoft Redmond,
	observing data mining analysts.  In that study, he took special
	note about how Microsoft's data scientists
	discussed the results of their data mining sessions with  business users. 
	
	One surprising observation was how  
	little time was spent by business users 
	inspecting  of the output of standard data miners. Prior to that visit,
	we had the mistaken impression that   decision trees,
	clustering algorithms, etc were useful ``off-the-shelf''; i.e    business
	users would inspect and understand the output of those tools.
	
Standard data mining tools are not necessarily the best tool for supporting that dialogue.
Menzies found that he had to do considerable work pre-processing data mining output
prior to the weekly briefing meetings for the business users. Initially,
that pre-processing was just clustering. This evolved into feature selection and finally
the a case-based reasoning tool called HOW.  


	
	as compared to another process, which we call {\em peeking}.
	In {\em peeking}, analysts and users spend much time
	inspecting and discussing small samples of either raw or exemplary or synthesized project data.  Further, very little of those discussions were  focused on classification
	(the addition of a labels to some unlabelled data). Rather, much time
	was spent in those meetings discussing {\em what to do next}; i.e. trying
	to determine what could be altered to better improve some business outcome.
	
	That   Microsoft  study found two common ``peeking'' methods.
	In {\em data engagement meetings},
	users debated the implications of data
	displayed on a screen. In this way, users
	engaged with the data and with each other by
	monitoring each others' queries and check each others'
	conclusions.
	
	Another data analysis pattern observed
	at Microsoft was  {\em cluster + contrast} in which
	data is  reduced to a few
	clusters. Users are then just shown the delta between those
	clusters. While contrasting, if feature values are
	the same in both clusters, then these were pruned from
	the reports to the user. In this way, very large
	data sets can be shown on one PowerPoint
	slide. Note that {\em cluster+contrast} is a tool that can be usefully employed within
	{\em data engagement meetings}.
	
	
	Cluster+contrast and engagement
	meetings are common practices at Microsoft. Yet  these methods had never been rigorously studied or certified.
	For both those reasons,
	we reflected over those tools to discover and analyze their
	underlying process. The result was HOW~\cite{howase}: a tool
	that combines (a)~feature selection; (b)~centroid generation from   clusters;
	(c)~contrast methods between centroids.
	While method (a) is widely used (e.g.~\cite{Menzies2010}),
	to the best of our knowledge, this combination of (abc) has not been thoroughly explored before.

This paper assesses two methods  for ``peeking'': a model-based method called DTREE and an instance-based method   called HOW. 
DTREE were first developed by Lekkalapudi and Menzies to  explaining results from multi-objective optimizers~\cite{nva14}. This paper is the first
to apply DTREE to defect prediction and contrast set learning. Also, that prior work evaluated
DTREE against multi-objective optimizers and not   instance-based methods.

This paper uses three criteria to assess the value of HOW and DTREE for
learning actionable analytics:
\be
\item
A planning system needs to be {\em effective}; i.e. if its recommendations
are applied then some statistically significant change should be observed.
To predict the number of defects in a data set before and after applying our changes,
we build a    prediction system (built by data mining; specifically: Random Forest). Note that this
predictor was built from some hold-out data (i.e. from  different data than that used
to build the predictor).
\item
Any  conclusion made to a business user must be understandable;
it must generate {\em succinct} changes.  
\item
Recommendations should be {\em stable}; i.e. they shouldn't widely vary due to minor changes in the data. Hence, in our experiments, we will add a little randomness to our analysis then report results across 20 repeated runs. 
\ee
We will find that
 {\em effectiveness} of DTREE and HOW are similar, but   DTREE wins on {\em succinctness} and {\em stability}.

\section{Cluster and Contrast}
\label{clust_contrast}
A recurring data analysis pattern in this paper is $cluster+contrast$. The data is distilled into a few clusters using a clustering scheme. Then lessons are inferred by studying the differences between these clusters. These lessons are used to generate rules that can be applied in any context.

\subsection{Clustering}
There are a wide variety of clustering methods to choose from. A study by Ganesan~\cite{div14} explored different clustering methods for software engineering data using the effort and defect data from the PROMISE repository~\cite{promise}. In that study methods such as WHERE, K-Means, mini-batch-K-Means, DBScan, EM, and Ward were investigated. The results of the study showed that the size and number of clusters is more important that the specifics of the techniques used. 

For this purposes of this work, we have chosen WHERE, a clustering scheme which is capable of generating at least $\sqrt{N}$ clusters given $N$ instances. In addition to this, WHERE has been shown to run fast while ignoring spurious dimensions~\cite{menzies2013}. This is particularly useful, for much of the SE data is noisy, they contain a information not associated with the target variable. 

\subsection{Finding Contrasts}
All the following methods use clustering in the form of WHERE, a top-down clustering method which recursively splits the data in two along a dimension that represents the highest variability. It works as follows:
\bi
\item Find   two   distance samples from the data, say  $X,Y$. This can be done by  picking any case $W$ at random, then setting $X$ to its most distant case, then setting $Y$ to the case most distant from $X$ (this requires only $O(2N)$ comparisons of $N$ cases).
\item Project each case $Z$ onto a {\tt Slope} that  runs between $X,Y$ using the cosine rule. 
\item Split the data at the median $X$ value of all cases and recurse on each half  (stopping when one half has less  than $\sqrt{N}$ of the original population).
\ei		

Clustering is followed by generating \textit{contrast sets}. These contrast sets represent recommendations on what could be altered to better improve an outcome. In this work we have explored several algorithms as possible tools to identify contrast between clusters. These fall into two broad categories:
\begin{enumerate}
\item Case Based Reasoning techniques (Nearest Neighbors and a gradient base planner called HOW)
\item Decision Trees.
\end{enumerate}

These techniques are discussed below. It is worth noting that the following techniques are organized in such a way that each one seeks to address certain shortcomings in the ones that precede it. 

\subsection{Case Based Reasoning}

Case-based reasoning seeks to find solutions to problems by emulating human recollection and adaptation from past experiences. It has found extensive usage in Artificial Intelligence because it offers several advantages. However, one of the most important benefits that CBR has to offer is that it works on a ``case-by-case'' basis. Therefore it's advise is tailored to be specific to the particular case being considered. Several paper in SE have applied this technique, most of all for effort estimation ~\cite{keung2008analogy, 6600685, walkerden1999empirical, shepperd1997estimating, kocaguneli2010use}. 

A classic example of a CBR is K Nearest Neighbor. Our version of nearest neighbor for planning is rather straight forward. It has been developed as a "straw-man"; i.e., a simple baseline tool to act as a benchmark used to evaluate other methods. 


\subsection{HOW}

HOW is very different compared to conventional CBR planners in that it explores the gradient between pairs of nearby clusters instead of studying the clusters themselves. HOW works by clustering the data during training using WHERE and then drawing \texttt{slopes} between the centroids of pairs of nearby clusters. Assuming the cluster pairs are labeled X and Y, with X having slightly better performance score than Y, the \texttt{slope} between X and Y acts as an indicator pointing to a direction in which to displace the data; i.e. away from Y and towards X. 

While testing, HOW finds the nearest slope to every test case. The slope provides the exact magnitude and direction of displacements. Contrast sets are derived from these displacements. HOW offers a distinct advantage over the other CBR planners by limiting the displacements to very small regions (the displacements are never more than the separation between two clusters). 

Although HOW manages to generate plans by localizing displacements to regions small enough to produce a statistically significant improvement, it needs to be noted that it fails to provide succinct summaries. Lack of succinctness makes it difficult draw generalizable conclusions about the test data. This is a trend commonly observed in most CBR systems. They tend to reason directly from a loaded training data instead of first summarizing the data into a model. 

% However, not all domains come with reliable models. For instance, a model that encompasses all the intricate issues that may lead to defects in software would be very large and indeed rather complex. In addition to this, finding empirical data to validate such models can be hard to come by and also time consuming ~\cite{me09i,me09j}. 

\subsection{DECISION TREES}

Trustworthy domain models are a popular alternatives to CBR planners. In our previous work, we have used executing source code as a "model" to check if our mutations to test suites minimize the test suite size while maximizing the number of statements covered~\cite{me09m,andrews07,andrews10}. However, we do not always have access to ready-to-use models. Further, finding empirical data to validate existing models can be hard. 

Taking into account the wealth of data that is available to us, this issue can fortunately be circumvented by constructing a categorical model based on decision trees. Unlike the previous planners which use where, here we construct a decision tree using information gain as a metric to find ideal splits in the data. This allows us to identify the most informative attributes to select. A decision tree (hereafter referred to as DTREE) built in this fashion emulates a model built from the training data. 

Using DTREE, the test cases can be categorized into one of the branches of the tree. Now, to generate the contrast sets, we determine (1) What \textit{current} branch does a test case fall in?; (2) What \textit{desired} branch would the test case have to move to?; (3) What are the deltas between \textit{current} and \textit{desired}? The last question can be answered by finding the deltas in branches of the decision tree that lead to \textit{desired} from \textit{current}. For full algorithmic details, refer to \fig{XTREE_bare}.

A tree structure such as DTREE is characterized by attributes such as size and depth. It is worth noting that these attributes have a profound impact on its performance. Our initial motivation for using DTREE was that they could serve as a medium for experts to reason about a data. The size of the tree, if too large, jeopardizes the readability of the solutions by increasing the complexity. We have, therefore, endeavored to reduce the size of the tree by pruning away irrelevant solutions that do not contribute to better solutions (refer to step-2 of \fig{XTREE_bare}). 

DTREE offers a great number of benefits compared to the other methods that were discussed above. Firstly, it offers a visual medium for experts to identify and explore solutions spaces that are local to the problem. Secondly, its solutions are much more stable than other cluster specific and instance specific learners. This stability can be attributed to the consistency and general reproducibility of the tree structure. Thirdly, and perhaps most importantly, the tree summarizes the training data succinctly, making it easier for a business user to examine the solution sets.

\lstnewenvironment{code}[1]{
\lstset{
mathescape,
numbers=left,
numberstyle=\scriptsize,
stepnumber=1,
numbersep=0.5em,
xleftmargin=1em,
framextopmargin=2em,
framexbottommargin=2em,
showspaces=false,
showtabs=false,
showstringspaces=false,
tabsize=2,
% Basic
basicstyle=\ttfamily\scriptsize,
backgroundcolor=\color{Background},
language=Python,
% Comments
commentstyle=\color{Comments}\slshape,
% Strings
stringstyle=\color{Strings},
morecomment=[s][\color{Strings}]{"""}{"""},
morecomment=[s][\color{Strings}]{'''}{'''},
% keywords
morekeywords={[1]import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert, dot, norm, zip, sorted},
keywordstyle={[1]\color{Code}\bfseries},
% additional keywords
morekeywords={[3]fastmap,Slope,bPruning,clister,train,leafs,weightedFeatures,HOW,envy, score,kNN,contrastSet,exemplar,Prune, FSel, exemplar,nearestSlope,dist,displace,geometry,splitAcross2Points,leaves,How,nearest,bPruning,Stats,divide,recurse,weight1,project, furthest, split,WHERE,clusterer, getContours,envied, fWeight, nearestContour, projection, mutate, HERE, knn},
keywordstyle={[3]\color{Keywords}\bfseries},
morekeywords={[2]@invari},
keywordstyle={[2]\color{Decorators}\slshape},
emph={self},
emphstyle={\color{self}\slshape},
firstnumber=last
%
}}{}
