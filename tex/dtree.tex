\begin{figure}[!t] 
	\begin{shaded}
  ~\hrule~
	
Using the training data,  divide the data using the decision tree of algorithm of \fig{where}.D into groups of
size $\alpha=\sqrt{N}$.

For each item in the test data,
	  find the {\em current } leaf: take each test instance, run it down to a leaf in the decision tree.  
After that,	  find the {\em desired} leaf:
		\begin{itemize}[leftmargin=3mm]
		\item Starting at {\em current}, ascend the tree $lvl\in \{0,1,2...\}$ levels;
		\item Identify {\em sibling} leaves; i.e. leaf clusters that can be reached from level $lvl$ that are not {\em current }
		\item Using the {\em score} defined above, find the {\em better} siblings; i.e. those with a {\em score} less than $\gamma=0.5$ times the mean score of {\em current}.
		 \bi
		 \item 
		   If none found, then repeat for $lvl += 1$
		 \item
		    Return no plan if the new $lvl$ is above the root.
		 \ei
		\item  Return the {\em closest} better sibling where distance is measured between the mean centroids of that sibling and {\em current}
		\ei
	 Also, find the {\em delta}; i.e. the set difference between  conditions in the decision tree branch to {\em desired} and {\em current}. To find that delta:
		\begin{itemize}[leftmargin=3mm]
		\item
		For discrete attributes,  return the value from {\em desired}. 
		\item
		For  numerics, return the numeric difference. 
		\item
		For numerics  discretized into ranges, return a random number selected from the low and high boundaries of the that range.
		\ei 
		Finally, return the delta as the plan for improving the test instance.
		~\hrule~
		\end{shaded}
		\caption{XTREES}		\label{fig:xtrees_bare}
	\end{figure}
