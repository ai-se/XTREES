\relax 
\bibstyle{unsrt}
\citation{div14}
\citation{promise}
\citation{menzies2013}
\citation{keung2008analogy,6600685,walkerden1999empirical,shepperd1997estimating,kocaguneli2010use}
\citation{Irani1993}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Cluster and Contrast}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Clustering}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Finding Contrasts}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Case Based Reasoning}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}HOW}{1}}
\bibstyle{plain}
\bibdata{References}
\bibcite{div14}{1}
\bibcite{promise}{2}
\bibcite{menzies2013}{3}
\bibcite{keung2008analogy}{4}
\bibcite{6600685}{5}
\bibcite{walkerden1999empirical}{6}
\bibcite{shepperd1997estimating}{7}
\bibcite{kocaguneli2010use}{8}
\bibcite{Irani1993}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces CROSSTREES. Controlled by the parameters $\{\alpha , \beta , \gamma , \delta , \epsilon \}$ (set via engineering judgement).\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:xtrees_bare}{{1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Decision Trees}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}CROSSTREES}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Design}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Assessment}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}When not to Plan?}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Results}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Threats to Validity}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}References}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and test {\em  data set properties} for the Jureczko data sets, sorted in ascending order of \% defective examples. On the right-hand-side, we show the {\em  results from learning}. Data is ``good'' if it has recall over 60\% and false alarm under 40\% (and note that, after tuning, there are more ``good'' than before). Data marked with ``$\star $'' show large improvements in performance, after tuning. Data marked with ``$\times $'' are ``not good'' since their test suites have so few non-defective examples (less than 5\% of the total sample) that it becomes harder to find better data towards which we can displace test data. \relax }}{3}}
\newlabel{fig:j}{{2}{3}}
